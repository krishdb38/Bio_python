{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "# import matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.size\"] = 16\n",
    "matplotlib.rcParams[\"figure.figsize\"]= (9,9)\n",
    "\n",
    "import seaborn as sns\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "#scipy helper functions\n",
    "from scipy.stats import percentileofscore\n",
    "from scipy import stats\n",
    "import glob\n",
    "\n",
    "### For Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Standard ML Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge                   \n",
    "from sklearn.linear_model import ElasticNet\n",
    "#splitting data into training/testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Metrix to verify the Prediction\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error , median_absolute_error\n",
    "#Distributions\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"../\")\n",
    "x = x.iloc[:,1:]\n",
    "y = pd.read_csv(\"ffyrgc1000.csv\")\n",
    "y = y.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see the sample of data\n",
    "display(x.head(2))\n",
    "display(y.head(2))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preparing data for training and testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train and test data,\n",
    "x_train,x_test,y_train, y_test= train_test_split(x,y,test_size=0.2) #80 % = 400 training % 100 test\n",
    "x_train.shape,y_train.shape, x_test.shape , y_test.shape\n",
    "#No suffling is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Bayesian Ridge from SKLEARN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_prediction = BayesianRidge(alpha_1=0,alpha_2=0,lambda_1=0,lambda_2=0).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data\n",
    "bay_prediction_ = bay_prediction.predict(x_train)\n",
    "print(stats.pearsonr(y_train.V1,bay_prediction_)[0])\n",
    "print(mean_absolute_error(y_train,bay_prediction_))\n",
    "print(mean_absolute_error(y_train,pd.DataFrame(bay_prediction_)))\n",
    "#bay_prediction.score(np.array(y_train.V1),np.array(y_train.V1))\n",
    "print(bay_prediction.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing Data\n",
    "bay_prediction_t = bay_prediction.predict(x_test)\n",
    "print(stats.pearsonr(y_test.V1,bay_prediction_t)[0])\n",
    "print(mean_absolute_error(y_test,bay_prediction_t))\n",
    "print(mean_absolute_error(y_test,pd.DataFrame(bay_prediction_t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(x_train,y_train)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "lr_prediction_ = lr.predict(x_train)\n",
    "print(stats.pearsonr(y_train.V1,pd.DataFrame(lr_prediction_)[0]))\n",
    "print(mean_absolute_error(y_train,lr_prediction_))\n",
    "#Although the Corelation shows 100 % But teh Mae and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "lr_prediction_t = lr.predict(x_test)\n",
    "from scipy import stats\n",
    "print(stats.pearsonr(y_test.V1,pd.DataFrame(lr_prediction_t)[0]))\n",
    "print(mean_absolute_error(y_test,lr_prediction_t))\n",
    "print(lr.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=0.62).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Prediction on Training data\n",
    "ridge_prediction_ = ridge.predict(x_train)\n",
    "print(stats.pearsonr(y_train.V1,pd.DataFrame(ridge_prediction_)[0]))\n",
    "print(mean_absolute_error(y_train,ridge_prediction_))\n",
    "print(mean_absolute_error(y_train,pd.DataFrame(ridge_prediction_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge Prediction on Testing data\n",
    "ridge_prediction_t = ridge.predict(x_test)\n",
    "print(stats.pearsonr(y_test.V1,pd.DataFrame(ridge_prediction_t)[0]))\n",
    "print(mean_absolute_error(y_test,ridge_prediction_t))\n",
    "print(mean_absolute_error(y_test,pd.DataFrame(ridge_prediction_t)))\n",
    "print(ridge.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet = ElasticNet().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enet Prediction on Training data\n",
    "enet_prediction_ = enet.predict(x_train)\n",
    "print(stats.pearsonr(y_train.V1,enet_prediction_))\n",
    "print(mean_absolute_error(y_train,enet_prediction_))\n",
    "print(mean_absolute_error(y_train,pd.DataFrame(enet_prediction_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enet Prediction on Training data\n",
    "enet_prediction_ = enet.predict(x_train)\n",
    "print(stats.pearsonr(y_train.V1,pd.DataFrame(enet_prediction_)[0]))\n",
    "print(mean_absolute_error(y_train,enet_prediction_))\n",
    "print(mean_absolute_error(y_train,pd.DataFrame(enet_prediction_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enet For Test data\n",
    "enet_prediction_t = enet.predict(x_test)\n",
    "print(stats.pearsonr(y_test.V1,pd.DataFrame(enet_prediction_t)[0]))\n",
    "print(mean_absolute_error(y_test,enet_prediction_t))\n",
    "print(mean_absolute_error(y_test,pd.DataFrame(enet_prediction_t)))\n",
    "print(enet.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Establish Benchmarks\n",
    "`Metrics`\n",
    "For this regression task, we will use two standard metrics:\n",
    "* Mean Absolute Error(MAE)\n",
    "* Root Mean Squared Errors(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sklearn.MLPRegressor()    =MultiLayer Perceptron regressor=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sklearn.neural_network.MLPRegressor(hidden_layer_size=(100,),activation = \"relu\",solver = \"adam\", alpha = 0.0001,\n",
    "    batch_size=\"auto\",learning_rate = \"constant\", learning_rate_init = 0.001, power_t = 0.5,max_iter = 200,shuffle = True,random_state = None, tol = 0.0001,verbose = False, warm_start = False, momentum=0.9,nesterovs_momentum = True, early_stopping = False, validation_fraction=0.1,beta_1 =0.9,beta_2 =0.999,epsilon = 1e-08,n_iter_no_change =10,max_fun = 15000)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor as mlp ## MLP Multi layer Perception\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = mlp(hidden_layer_sizes=((93)),\\\n",
    "            activation= 'logistic',\\\n",
    "            solver=\"lbfgs\",\\\n",
    "            alpha=0.63,\\\n",
    "            batch_size=\"auto\",\\\n",
    "            learning_rate=\"adaptive\",\\\n",
    "            power_t=0.05,\\\n",
    "            max_iter=200,\\\n",
    "            shuffle=True,\\\n",
    "            random_state=False,\\\n",
    "            tol=0.0005,\\\n",
    "            verbose=False,\\\n",
    "            warm_start=False,\\\n",
    "            momentum=0.9,\\\n",
    "            nesterovs_momentum=True,\\\n",
    "            early_stopping=False,\\\n",
    "            validation_fraction=0,\\\n",
    "            beta_1=0.9,\\\n",
    "            beta_2=0.999,\\\n",
    "            epsilon=1e-08,\\\n",
    "            n_iter_no_change=10,\\\n",
    "            max_fun=15000,\\\n",
    "            learning_rate_init= 0.05,\\\n",
    "            )\n",
    "#best are 93 , 108 ,147 ,\n",
    "#alfa 0.63\n",
    "mlp_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training \n",
    "mlp_prediction_  = mlp_model.predict(x_train)\n",
    "print(stats.pearsonr(y_train.V1,mlp_prediction_)[0])\n",
    "print(mean_absolute_error(y_train,mlp_prediction_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "mlp_prediction_t  = mlp_model.predict(x_test)\n",
    "print(stats.pearsonr(y_test.V1,mlp_prediction_t)[0])\n",
    "print(mean_absolute_error(y_test,mlp_prediction_t))\n",
    "print(mlp_model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop testing for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = ['identity', 'logistic', \"tanh\", \"relu\"]\n",
    "solver = [\"lbfgs\",'sgd','adam']\n",
    "learning_rate = ['constant','invscaling',\"adaptive\"]\n",
    "for _ in learning_rate:\n",
    "    mlp_model_ = mlp(hidden_layer_sizes=((93,)),\n",
    "                          activation=\"logistic\",\n",
    "                          solver=\"lbfgs\",\n",
    "                          learning_rate=_,\n",
    "                          shuffle=True,\n",
    "                          random_state=False,\n",
    "                          alpha=0.63,\n",
    "                          learning_rate_init=0.05,\n",
    "                          max_iter=200,\n",
    "                          tol=0.0005)\n",
    "    mlp_model_.fit(x_train,y_train)\n",
    "    mlp_pred = mlp_model_.predict(x_test) \n",
    "    print(mlp_model_.score(x_test,y_test),_)\n",
    "    #print(stats.pearsonr(y_test.V1,mlp_pred)[0],end=\"-\")\n",
    "    #print(_)\n",
    "## 66.65 is the highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CRTEATING A DATA FRAME FOR ALL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = y_test[:10].reset_index(drop=True)#,\n",
    "bay= pd.DataFrame(bay_prediction_t[:10],columns=[\"Bayesian\"])\n",
    "lr= pd.DataFrame(lr_prediction_t[:10],columns=['Linear_Regression'])\n",
    "ridg = pd.DataFrame(ridge_prediction_t[:10],columns=['Ridge_Regression'])\n",
    "ent = pd.DataFrame(enet_prediction_t[:10],columns=[\"Enet_Regression\"])\n",
    "ml = pd.DataFrame(mlp_prediction_t[:10],columns=['MLP_Neural'])\n",
    "#pd.merge(y_test[:10],(),how = \"outer\")\n",
    "frame = [original,bay,lr,ridg,ent,ml]\n",
    "pd.concat(frame,axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
